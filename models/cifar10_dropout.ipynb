{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, utils\n",
    "import lenet5_cifar10 as models\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train :  40000\n",
      "Validation :  10000\n",
      "Test :  10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "root = \"./CIFAR10_DATASET\"\n",
    "train_dataset = datasets.CIFAR10(root, transform=transform, train=True, download=True)\n",
    "train_dataset, valid_dataset = utils.data.random_split(train_dataset, [40000, 10000])\n",
    "test_dataset = datasets.CIFAR10(root, transform=transform, train=False, download=True)\n",
    "\n",
    "print(\"Train : \", len(train_dataset))\n",
    "print(\"Validation : \", len(valid_dataset))\n",
    "print(\"Test : \", len(test_dataset))\n",
    "\n",
    "train_batchsize = 64\n",
    "test_batchsize = 256\n",
    "\n",
    "train_dataloader = utils.data.DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True)\n",
    "valid_dataloader = utils.data.DataLoader(valid_dataset, batch_size=test_batchsize, shuffle=False)\n",
    "test_dataloader = utils.data.DataLoader(test_dataset, batch_size=test_batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_experiment(bnn_model, train_dataloader, valid_dataloader, use_aleatoric=True, **kwargs):\n",
    "  if bnn_model == \"gaussian\":\n",
    "    model = models.gaussian_lenet5(kwargs[\"var_type\"], dict(), kwargs[\"is_lrt\"], use_aleatoric)\n",
    "    optimizer = optim.SGD(model.parameters(), kwargs[\"lr\"], kwargs[\"momentum\"])\n",
    "    # Every forward gives different output.\n",
    "    bnn_type = \"random\"\n",
    "  elif bnn_model == \"dropout\":\n",
    "    model = models.dropout_lenet5(kwargs[\"dropout_rate\"], kwargs[\"dropout_type\"], dict(), use_aleatoric)\n",
    "    model = model.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), kwargs[\"lr\"], kwargs[\"momentum\"], weight_decay=(1 - kwargs[\"dropout_rate\"]) / (2 * kwargs[\"num_sample\"] * 10))\n",
    "    # Every forward gives different output.\n",
    "    bnn_type = \"random\"\n",
    "  elif bnn_model == \"ensemble\":\n",
    "    model = models.ensemble_lenet5(kwargs[\"num_ensemble\"], use_aleatoric)\n",
    "    model = model.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), kwargs[\"lr\"], kwargs[\"momentum\"])\n",
    "    # Forward gives [batch_size * num_ensemble, output_shape]\n",
    "    bnn_type = \"ensemble\"\n",
    "  elif bnn_model == \"swag\":\n",
    "    model = models.lenet5(use_aleatoric)\n",
    "    model = model.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), kwargs[\"lr\"], kwargs[\"momentum\"])\n",
    "    # SWAG is trained as simple NN.\n",
    "    bnn_type = \"swag\"\n",
    "  elif bnn_model == \"batchensemble\":\n",
    "    model = models.batchensemble_lenet5(kwargs[\"num_models\"], use_aleatoric)\n",
    "    model = model.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), kwargs[\"lr\"], kwargs[\"momentum\"])\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.1 ** (1/100))\n",
    "    # Forward gives [batch_size * num_models, output_shape]\n",
    "    bnn_type = \"ensemble\"\n",
    "    kwargs[\"num_ensemble\"] = kwargs[\"num_models\"]\n",
    "  else:\n",
    "    raise ValueError(\"No bnn model choosen.\")\n",
    "  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.1 ** (1/100))\n",
    "  \n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  train_loss_res = []\n",
    "  valid_loss_res = []\n",
    "  train_acc_res = []\n",
    "  valid_acc_res = []\n",
    "  for epoch in range(kwargs[\"epoch\"]):\n",
    "    train_loss = 0.0\n",
    "    train_acc_count = 0\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "      images, labels = data\n",
    "      images = images.cuda()\n",
    "      labels = labels.cuda()\n",
    "      optimizer.zero_grad()\n",
    "      if bnn_type == \"random\":\n",
    "        loss = 0\n",
    "        for ind_sample in range(kwargs[\"num_sample\"]):\n",
    "          outputs = model(images)\n",
    "          if use_aleatoric:\n",
    "            output_mean, output_std = torch.chunk(outputs, 2, dim=1)\n",
    "            eps = torch.normal(0, 1, output_mean.shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            outputs = output_mean + eps * output_std\n",
    "          output_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "          train_acc_count += torch.count_nonzero(output_pred == labels).item() / kwargs[\"num_sample\"]\n",
    "          loss += criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() / kwargs[\"num_sample\"]\n",
    "          \n",
    "      elif bnn_type == \"ensemble\":\n",
    "        outputs = model(images)\n",
    "        if use_aleatoric:\n",
    "          output_mean, output_std = torch.chunk(outputs, 2, dim=1)\n",
    "          eps = torch.normal(0, 1, output_mean.shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "          outputs = output_mean + eps * output_std\n",
    "        labels = labels.repeat(kwargs[\"num_ensemble\"]) # [y1, y2, ..., y1, y2, ..., ] with num_ensemble times\n",
    "        loss = criterion(outputs, labels) * kwargs[\"num_ensemble\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() / kwargs[\"num_ensemble\"]\n",
    "        output_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "        train_acc_count += torch.count_nonzero(output_pred == labels).item() / kwargs[\"num_ensemble\"]\n",
    "      else:\n",
    "        outputs = model(images)\n",
    "        if use_aleatoric:\n",
    "          output_mean, output_std = torch.chunk(outputs, 2, dim=1)\n",
    "          eps = torch.normal(0, 1, output_mean.shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "          outputs = output_mean + eps * output_std\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        output_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "        train_acc_count += torch.count_nonzero(output_pred == labels).item()\n",
    "    train_loss_res.append(train_loss)\n",
    "    train_acc_res.append(train_acc_count / len(train_dataset))\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc_count = 0\n",
    "    with torch.no_grad():\n",
    "      for data in valid_dataloader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        if bnn_type == \"random\":\n",
    "          for ind_sample in range(kwargs[\"valid_num_sample\"]):\n",
    "            outputs = model(images)\n",
    "            if use_aleatoric:\n",
    "              output_mean, output_std = torch.chunk(outputs, 2, dim=1)\n",
    "              eps = torch.normal(0, 1, output_mean.shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "              outputs = output_mean + eps * output_std\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            valid_loss += loss.item() / kwargs[\"valid_num_sample\"]\n",
    "            output_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "            valid_acc_count += torch.count_nonzero(output_pred == labels).item() / kwargs[\"valid_num_sample\"]\n",
    "        elif bnn_type == \"ensemble\":\n",
    "          outputs = model(images)\n",
    "          if use_aleatoric:\n",
    "            output_mean, output_std = torch.chunk(outputs, 2, dim=1)\n",
    "            eps = torch.normal(0, 1, output_mean.shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            outputs = output_mean + eps * output_std\n",
    "          labels = labels.repeat(kwargs[\"num_ensemble\"]) # [y1, y2, ..., y1, y2, ..., ] with num_ensemble times\n",
    "          loss = criterion(outputs, labels) * kwargs[\"num_ensemble\"]\n",
    "          valid_loss += loss.item() / kwargs[\"num_ensemble\"]\n",
    "          output_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "          valid_acc_count += torch.count_nonzero(output_pred == labels).item() / kwargs[\"num_ensemble\"]\n",
    "        else:\n",
    "          outputs = model(images)\n",
    "          if use_aleatoric:\n",
    "            output_mean, output_std = torch.chunk(outputs, 2, dim=1)\n",
    "            eps = torch.normal(0, 1, output_mean.shape, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            outputs = output_mean + eps * output_std\n",
    "          loss = criterion(outputs, labels)\n",
    "          valid_loss += loss.item()\n",
    "          output_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "          valid_acc_count += torch.count_nonzero(output_pred == labels).item()\n",
    "    valid_loss_res.append(valid_loss)\n",
    "    valid_acc_res.append(valid_acc_count / len(valid_dataset))\n",
    "    print(f\"Epoch {epoch+1} ended\")\n",
    "    print(f\"Training   loss/acc : {train_loss:.3f}/{train_acc_count / len(train_dataset):.3f}\")\n",
    "    print(f\"Validation loss/acc : {valid_loss:.3f}/{valid_acc_count / len(valid_dataset):.3f}\")\n",
    "  \n",
    "  return model, train_loss_res, train_acc_res, valid_loss_res, valid_acc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ended\n",
      "Training   loss/acc : 1316.858/0.214\n",
      "Validation loss/acc : 77.606/0.277\n",
      "Epoch 2 ended\n",
      "Training   loss/acc : 1210.018/0.294\n",
      "Validation loss/acc : 76.893/0.306\n",
      "Epoch 3 ended\n",
      "Training   loss/acc : 1161.193/0.327\n",
      "Validation loss/acc : 71.945/0.348\n",
      "Epoch 4 ended\n",
      "Training   loss/acc : 1133.600/0.345\n",
      "Validation loss/acc : 69.929/0.377\n",
      "Epoch 5 ended\n",
      "Training   loss/acc : 1098.421/0.364\n",
      "Validation loss/acc : 71.279/0.359\n",
      "Epoch 6 ended\n",
      "Training   loss/acc : 1071.249/0.375\n",
      "Validation loss/acc : 69.215/0.371\n",
      "Epoch 7 ended\n",
      "Training   loss/acc : 1046.353/0.394\n",
      "Validation loss/acc : 68.133/0.389\n",
      "Epoch 8 ended\n",
      "Training   loss/acc : 1024.819/0.406\n",
      "Validation loss/acc : 66.114/0.412\n",
      "Epoch 9 ended\n",
      "Training   loss/acc : 1003.203/0.418\n",
      "Validation loss/acc : 65.713/0.410\n",
      "Epoch 10 ended\n",
      "Training   loss/acc : 987.155/0.429\n",
      "Validation loss/acc : 63.090/0.436\n",
      "Epoch 11 ended\n",
      "Training   loss/acc : 967.304/0.438\n",
      "Validation loss/acc : 63.520/0.431\n",
      "Epoch 12 ended\n",
      "Training   loss/acc : 959.180/0.446\n",
      "Validation loss/acc : 59.997/0.464\n",
      "Epoch 13 ended\n",
      "Training   loss/acc : 938.638/0.461\n",
      "Validation loss/acc : 62.651/0.439\n",
      "Epoch 14 ended\n",
      "Training   loss/acc : 930.238/0.467\n",
      "Validation loss/acc : 60.554/0.461\n",
      "Epoch 15 ended\n",
      "Training   loss/acc : 918.142/0.475\n",
      "Validation loss/acc : 56.727/0.491\n",
      "Epoch 16 ended\n",
      "Training   loss/acc : 897.591/0.488\n",
      "Validation loss/acc : 55.961/0.502\n",
      "Epoch 17 ended\n",
      "Training   loss/acc : 889.976/0.493\n",
      "Validation loss/acc : 56.912/0.495\n",
      "Epoch 18 ended\n",
      "Training   loss/acc : 877.575/0.500\n",
      "Validation loss/acc : 53.967/0.522\n",
      "Epoch 19 ended\n",
      "Training   loss/acc : 860.376/0.511\n",
      "Validation loss/acc : 53.800/0.529\n",
      "Epoch 20 ended\n",
      "Training   loss/acc : 852.919/0.517\n",
      "Validation loss/acc : 54.159/0.526\n",
      "Epoch 21 ended\n",
      "Training   loss/acc : 839.604/0.524\n",
      "Validation loss/acc : 53.677/0.532\n",
      "Epoch 22 ended\n",
      "Training   loss/acc : 826.169/0.532\n",
      "Validation loss/acc : 52.056/0.545\n",
      "Epoch 23 ended\n",
      "Training   loss/acc : 821.509/0.535\n",
      "Validation loss/acc : 50.125/0.559\n",
      "Epoch 24 ended\n",
      "Training   loss/acc : 820.004/0.538\n",
      "Validation loss/acc : 53.218/0.535\n",
      "Epoch 25 ended\n",
      "Training   loss/acc : 815.041/0.539\n",
      "Validation loss/acc : 54.682/0.527\n",
      "Epoch 26 ended\n",
      "Training   loss/acc : 798.258/0.549\n",
      "Validation loss/acc : 51.078/0.554\n",
      "Epoch 27 ended\n",
      "Training   loss/acc : 793.270/0.553\n",
      "Validation loss/acc : 50.078/0.566\n",
      "Epoch 28 ended\n",
      "Training   loss/acc : 781.833/0.558\n",
      "Validation loss/acc : 49.922/0.557\n",
      "Epoch 29 ended\n",
      "Training   loss/acc : 778.071/0.561\n",
      "Validation loss/acc : 53.362/0.541\n",
      "Epoch 30 ended\n",
      "Training   loss/acc : 774.580/0.563\n",
      "Validation loss/acc : 48.742/0.575\n",
      "Epoch 31 ended\n",
      "Training   loss/acc : 766.513/0.568\n",
      "Validation loss/acc : 52.126/0.548\n",
      "Epoch 32 ended\n",
      "Training   loss/acc : 762.231/0.572\n",
      "Validation loss/acc : 50.518/0.553\n",
      "Epoch 33 ended\n",
      "Training   loss/acc : 750.578/0.576\n",
      "Validation loss/acc : 50.798/0.554\n",
      "Epoch 34 ended\n",
      "Training   loss/acc : 744.975/0.581\n",
      "Validation loss/acc : 49.083/0.573\n",
      "Epoch 35 ended\n",
      "Training   loss/acc : 735.761/0.586\n",
      "Validation loss/acc : 50.976/0.563\n",
      "Epoch 36 ended\n",
      "Training   loss/acc : 725.351/0.593\n",
      "Validation loss/acc : 48.317/0.575\n",
      "Epoch 37 ended\n",
      "Training   loss/acc : 719.917/0.594\n",
      "Validation loss/acc : 51.288/0.552\n",
      "Epoch 38 ended\n",
      "Training   loss/acc : 711.958/0.599\n",
      "Validation loss/acc : 45.583/0.603\n",
      "Epoch 39 ended\n",
      "Training   loss/acc : 709.583/0.600\n",
      "Validation loss/acc : 47.619/0.586\n",
      "Epoch 40 ended\n",
      "Training   loss/acc : 692.587/0.610\n",
      "Validation loss/acc : 43.998/0.616\n",
      "Epoch 41 ended\n",
      "Training   loss/acc : 689.151/0.614\n",
      "Validation loss/acc : 44.211/0.620\n",
      "Epoch 42 ended\n",
      "Training   loss/acc : 674.708/0.622\n",
      "Validation loss/acc : 43.636/0.619\n",
      "Epoch 43 ended\n",
      "Training   loss/acc : 673.768/0.622\n",
      "Validation loss/acc : 43.656/0.621\n",
      "Epoch 44 ended\n",
      "Training   loss/acc : 666.195/0.628\n",
      "Validation loss/acc : 43.833/0.622\n",
      "Epoch 45 ended\n",
      "Training   loss/acc : 650.846/0.637\n",
      "Validation loss/acc : 42.294/0.633\n",
      "Epoch 46 ended\n",
      "Training   loss/acc : 649.846/0.637\n",
      "Validation loss/acc : 47.600/0.587\n",
      "Epoch 47 ended\n",
      "Training   loss/acc : 645.458/0.641\n",
      "Validation loss/acc : 42.580/0.628\n",
      "Epoch 48 ended\n",
      "Training   loss/acc : 635.166/0.647\n",
      "Validation loss/acc : 45.151/0.609\n",
      "Epoch 49 ended\n",
      "Training   loss/acc : 634.924/0.647\n",
      "Validation loss/acc : 41.582/0.640\n",
      "Epoch 50 ended\n",
      "Training   loss/acc : 631.179/0.648\n",
      "Validation loss/acc : 44.205/0.614\n",
      "Epoch 51 ended\n",
      "Training   loss/acc : 623.967/0.651\n",
      "Validation loss/acc : 44.628/0.610\n",
      "Epoch 52 ended\n",
      "Training   loss/acc : 625.491/0.653\n",
      "Validation loss/acc : 42.056/0.631\n",
      "Epoch 53 ended\n",
      "Training   loss/acc : 611.764/0.659\n",
      "Validation loss/acc : 40.699/0.647\n",
      "Epoch 54 ended\n",
      "Training   loss/acc : 612.208/0.659\n",
      "Validation loss/acc : 41.223/0.643\n",
      "Epoch 55 ended\n",
      "Training   loss/acc : 604.347/0.664\n",
      "Validation loss/acc : 41.619/0.640\n",
      "Epoch 56 ended\n",
      "Training   loss/acc : 598.865/0.667\n",
      "Validation loss/acc : 39.316/0.657\n",
      "Epoch 57 ended\n",
      "Training   loss/acc : 594.532/0.670\n",
      "Validation loss/acc : 41.207/0.643\n",
      "Epoch 58 ended\n",
      "Training   loss/acc : 595.185/0.670\n",
      "Validation loss/acc : 41.614/0.643\n",
      "Epoch 59 ended\n",
      "Training   loss/acc : 587.793/0.675\n",
      "Validation loss/acc : 41.456/0.642\n",
      "Epoch 60 ended\n",
      "Training   loss/acc : 584.532/0.673\n",
      "Validation loss/acc : 39.796/0.660\n",
      "Epoch 61 ended\n",
      "Training   loss/acc : 581.687/0.677\n",
      "Validation loss/acc : 38.704/0.665\n",
      "Epoch 62 ended\n",
      "Training   loss/acc : 576.894/0.678\n",
      "Validation loss/acc : 41.155/0.644\n",
      "Epoch 63 ended\n",
      "Training   loss/acc : 580.411/0.679\n",
      "Validation loss/acc : 38.936/0.665\n",
      "Epoch 64 ended\n",
      "Training   loss/acc : 572.023/0.682\n",
      "Validation loss/acc : 39.023/0.661\n",
      "Epoch 65 ended\n",
      "Training   loss/acc : 570.811/0.684\n",
      "Validation loss/acc : 40.955/0.643\n",
      "Epoch 66 ended\n",
      "Training   loss/acc : 566.981/0.684\n",
      "Validation loss/acc : 40.260/0.653\n",
      "Epoch 67 ended\n",
      "Training   loss/acc : 559.333/0.690\n",
      "Validation loss/acc : 39.742/0.653\n",
      "Epoch 68 ended\n",
      "Training   loss/acc : 558.051/0.690\n",
      "Validation loss/acc : 39.345/0.653\n",
      "Epoch 69 ended\n",
      "Training   loss/acc : 559.123/0.690\n",
      "Validation loss/acc : 39.294/0.662\n",
      "Epoch 70 ended\n",
      "Training   loss/acc : 553.868/0.692\n",
      "Validation loss/acc : 38.292/0.672\n",
      "Epoch 71 ended\n",
      "Training   loss/acc : 548.953/0.696\n",
      "Validation loss/acc : 38.321/0.670\n",
      "Epoch 72 ended\n",
      "Training   loss/acc : 549.979/0.695\n",
      "Validation loss/acc : 39.089/0.664\n",
      "Epoch 73 ended\n",
      "Training   loss/acc : 543.004/0.698\n",
      "Validation loss/acc : 39.617/0.661\n",
      "Epoch 74 ended\n",
      "Training   loss/acc : 538.689/0.702\n",
      "Validation loss/acc : 37.809/0.677\n",
      "Epoch 75 ended\n",
      "Training   loss/acc : 536.732/0.702\n",
      "Validation loss/acc : 37.949/0.669\n",
      "Epoch 76 ended\n",
      "Training   loss/acc : 532.706/0.704\n",
      "Validation loss/acc : 38.710/0.667\n",
      "Epoch 77 ended\n",
      "Training   loss/acc : 529.931/0.706\n",
      "Validation loss/acc : 38.332/0.668\n",
      "Epoch 78 ended\n",
      "Training   loss/acc : 529.887/0.706\n",
      "Validation loss/acc : 37.434/0.678\n",
      "Epoch 79 ended\n",
      "Training   loss/acc : 524.801/0.709\n",
      "Validation loss/acc : 38.006/0.672\n",
      "Epoch 80 ended\n",
      "Training   loss/acc : 523.616/0.710\n",
      "Validation loss/acc : 36.514/0.682\n",
      "Epoch 81 ended\n",
      "Training   loss/acc : 521.880/0.711\n",
      "Validation loss/acc : 36.585/0.686\n",
      "Epoch 82 ended\n",
      "Training   loss/acc : 512.772/0.715\n",
      "Validation loss/acc : 36.430/0.684\n",
      "Epoch 83 ended\n",
      "Training   loss/acc : 511.904/0.716\n",
      "Validation loss/acc : 37.789/0.679\n",
      "Epoch 84 ended\n",
      "Training   loss/acc : 513.488/0.715\n",
      "Validation loss/acc : 36.148/0.686\n",
      "Epoch 85 ended\n",
      "Training   loss/acc : 506.810/0.717\n",
      "Validation loss/acc : 38.913/0.668\n",
      "Epoch 86 ended\n",
      "Training   loss/acc : 503.304/0.720\n",
      "Validation loss/acc : 36.823/0.688\n",
      "Epoch 87 ended\n",
      "Training   loss/acc : 499.902/0.721\n",
      "Validation loss/acc : 37.703/0.673\n",
      "Epoch 88 ended\n",
      "Training   loss/acc : 502.270/0.721\n",
      "Validation loss/acc : 36.243/0.688\n",
      "Epoch 89 ended\n",
      "Training   loss/acc : 497.050/0.724\n",
      "Validation loss/acc : 36.123/0.689\n",
      "Epoch 90 ended\n",
      "Training   loss/acc : 493.656/0.726\n",
      "Validation loss/acc : 36.206/0.690\n",
      "Epoch 91 ended\n",
      "Training   loss/acc : 492.739/0.725\n",
      "Validation loss/acc : 38.449/0.675\n",
      "Epoch 92 ended\n",
      "Training   loss/acc : 487.118/0.730\n",
      "Validation loss/acc : 35.197/0.697\n",
      "Epoch 93 ended\n",
      "Training   loss/acc : 482.856/0.733\n",
      "Validation loss/acc : 38.900/0.666\n",
      "Epoch 94 ended\n",
      "Training   loss/acc : 485.152/0.730\n",
      "Validation loss/acc : 36.300/0.687\n",
      "Epoch 95 ended\n",
      "Training   loss/acc : 479.843/0.734\n",
      "Validation loss/acc : 36.210/0.690\n",
      "Epoch 96 ended\n",
      "Training   loss/acc : 479.691/0.733\n",
      "Validation loss/acc : 36.168/0.688\n",
      "Epoch 97 ended\n",
      "Training   loss/acc : 474.400/0.737\n",
      "Validation loss/acc : 35.728/0.693\n",
      "Epoch 98 ended\n",
      "Training   loss/acc : 473.858/0.736\n",
      "Validation loss/acc : 35.672/0.695\n",
      "Epoch 99 ended\n",
      "Training   loss/acc : 471.374/0.739\n",
      "Validation loss/acc : 35.628/0.693\n",
      "Epoch 100 ended\n",
      "Training   loss/acc : 467.987/0.740\n",
      "Validation loss/acc : 36.138/0.691\n",
      "Epoch 101 ended\n",
      "Training   loss/acc : 465.174/0.742\n",
      "Validation loss/acc : 34.751/0.699\n",
      "Epoch 102 ended\n",
      "Training   loss/acc : 461.322/0.743\n",
      "Validation loss/acc : 36.200/0.693\n",
      "Epoch 103 ended\n",
      "Training   loss/acc : 465.211/0.742\n",
      "Validation loss/acc : 35.389/0.696\n",
      "Epoch 104 ended\n",
      "Training   loss/acc : 456.483/0.746\n",
      "Validation loss/acc : 37.329/0.686\n",
      "Epoch 105 ended\n",
      "Training   loss/acc : 454.237/0.748\n",
      "Validation loss/acc : 34.944/0.701\n",
      "Epoch 106 ended\n",
      "Training   loss/acc : 453.095/0.748\n",
      "Validation loss/acc : 34.727/0.703\n",
      "Epoch 107 ended\n",
      "Training   loss/acc : 450.343/0.751\n",
      "Validation loss/acc : 35.625/0.693\n",
      "Epoch 108 ended\n",
      "Training   loss/acc : 449.123/0.751\n",
      "Validation loss/acc : 36.733/0.684\n",
      "Epoch 109 ended\n",
      "Training   loss/acc : 449.097/0.750\n",
      "Validation loss/acc : 34.814/0.704\n",
      "Epoch 110 ended\n",
      "Training   loss/acc : 443.994/0.753\n",
      "Validation loss/acc : 35.915/0.693\n",
      "Epoch 111 ended\n",
      "Training   loss/acc : 441.033/0.755\n",
      "Validation loss/acc : 35.449/0.696\n",
      "Epoch 112 ended\n",
      "Training   loss/acc : 437.906/0.756\n",
      "Validation loss/acc : 36.209/0.691\n",
      "Epoch 113 ended\n",
      "Training   loss/acc : 436.222/0.757\n",
      "Validation loss/acc : 34.965/0.702\n",
      "Epoch 114 ended\n",
      "Training   loss/acc : 431.770/0.761\n",
      "Validation loss/acc : 35.737/0.696\n",
      "Epoch 115 ended\n",
      "Training   loss/acc : 433.028/0.760\n",
      "Validation loss/acc : 34.918/0.700\n",
      "Epoch 116 ended\n",
      "Training   loss/acc : 430.626/0.761\n",
      "Validation loss/acc : 35.542/0.694\n",
      "Epoch 117 ended\n",
      "Training   loss/acc : 428.026/0.763\n",
      "Validation loss/acc : 35.480/0.699\n",
      "Epoch 118 ended\n",
      "Training   loss/acc : 427.363/0.762\n",
      "Validation loss/acc : 36.888/0.688\n",
      "Epoch 119 ended\n",
      "Training   loss/acc : 421.651/0.764\n",
      "Validation loss/acc : 34.444/0.704\n",
      "Epoch 120 ended\n",
      "Training   loss/acc : 420.491/0.767\n",
      "Validation loss/acc : 34.738/0.703\n",
      "Epoch 121 ended\n",
      "Training   loss/acc : 419.432/0.767\n",
      "Validation loss/acc : 34.434/0.704\n",
      "Epoch 122 ended\n",
      "Training   loss/acc : 416.418/0.768\n",
      "Validation loss/acc : 34.684/0.703\n",
      "Epoch 123 ended\n",
      "Training   loss/acc : 415.581/0.769\n",
      "Validation loss/acc : 33.819/0.712\n",
      "Epoch 124 ended\n",
      "Training   loss/acc : 411.830/0.771\n",
      "Validation loss/acc : 34.575/0.707\n",
      "Epoch 125 ended\n",
      "Training   loss/acc : 409.854/0.771\n",
      "Validation loss/acc : 35.088/0.705\n",
      "Epoch 126 ended\n",
      "Training   loss/acc : 405.691/0.775\n",
      "Validation loss/acc : 35.198/0.701\n",
      "Epoch 127 ended\n",
      "Training   loss/acc : 407.836/0.773\n",
      "Validation loss/acc : 33.957/0.709\n",
      "Epoch 128 ended\n",
      "Training   loss/acc : 403.624/0.776\n",
      "Validation loss/acc : 34.289/0.710\n",
      "Epoch 129 ended\n",
      "Training   loss/acc : 401.349/0.776\n",
      "Validation loss/acc : 34.987/0.702\n",
      "Epoch 130 ended\n",
      "Training   loss/acc : 399.958/0.779\n",
      "Validation loss/acc : 33.688/0.714\n",
      "Epoch 131 ended\n",
      "Training   loss/acc : 400.028/0.778\n",
      "Validation loss/acc : 35.042/0.702\n",
      "Epoch 132 ended\n",
      "Training   loss/acc : 393.365/0.781\n",
      "Validation loss/acc : 34.231/0.706\n",
      "Epoch 133 ended\n",
      "Training   loss/acc : 391.421/0.782\n",
      "Validation loss/acc : 34.323/0.708\n",
      "Epoch 134 ended\n",
      "Training   loss/acc : 391.909/0.783\n",
      "Validation loss/acc : 35.155/0.705\n",
      "Epoch 135 ended\n",
      "Training   loss/acc : 389.776/0.783\n",
      "Validation loss/acc : 34.878/0.705\n",
      "Epoch 136 ended\n",
      "Training   loss/acc : 387.396/0.785\n",
      "Validation loss/acc : 35.422/0.698\n",
      "Epoch 137 ended\n",
      "Training   loss/acc : 387.183/0.785\n",
      "Validation loss/acc : 34.978/0.702\n",
      "Epoch 138 ended\n",
      "Training   loss/acc : 386.603/0.786\n",
      "Validation loss/acc : 33.406/0.715\n",
      "Epoch 139 ended\n",
      "Training   loss/acc : 382.955/0.788\n",
      "Validation loss/acc : 34.444/0.708\n",
      "Epoch 140 ended\n",
      "Training   loss/acc : 380.836/0.789\n",
      "Validation loss/acc : 34.505/0.706\n",
      "Epoch 141 ended\n",
      "Training   loss/acc : 379.416/0.790\n",
      "Validation loss/acc : 34.932/0.703\n",
      "Epoch 142 ended\n",
      "Training   loss/acc : 377.666/0.789\n",
      "Validation loss/acc : 34.045/0.711\n",
      "Epoch 143 ended\n",
      "Training   loss/acc : 375.337/0.791\n",
      "Validation loss/acc : 34.883/0.707\n",
      "Epoch 144 ended\n",
      "Training   loss/acc : 374.354/0.791\n",
      "Validation loss/acc : 34.265/0.710\n",
      "Epoch 145 ended\n",
      "Training   loss/acc : 375.068/0.791\n",
      "Validation loss/acc : 34.327/0.710\n",
      "Epoch 146 ended\n",
      "Training   loss/acc : 371.522/0.795\n",
      "Validation loss/acc : 34.229/0.711\n",
      "Epoch 147 ended\n",
      "Training   loss/acc : 370.856/0.793\n",
      "Validation loss/acc : 34.401/0.709\n",
      "Epoch 148 ended\n",
      "Training   loss/acc : 369.894/0.794\n",
      "Validation loss/acc : 34.450/0.709\n",
      "Epoch 149 ended\n",
      "Training   loss/acc : 364.805/0.797\n",
      "Validation loss/acc : 35.499/0.701\n",
      "Epoch 150 ended\n",
      "Training   loss/acc : 364.367/0.797\n",
      "Validation loss/acc : 34.106/0.712\n",
      "Epoch 151 ended\n",
      "Training   loss/acc : 363.338/0.797\n",
      "Validation loss/acc : 34.375/0.708\n",
      "Epoch 152 ended\n",
      "Training   loss/acc : 361.364/0.800\n",
      "Validation loss/acc : 34.893/0.705\n",
      "Epoch 153 ended\n",
      "Training   loss/acc : 361.797/0.798\n",
      "Validation loss/acc : 35.515/0.702\n",
      "Epoch 154 ended\n",
      "Training   loss/acc : 359.764/0.799\n",
      "Validation loss/acc : 34.297/0.710\n",
      "Epoch 155 ended\n",
      "Training   loss/acc : 356.450/0.801\n",
      "Validation loss/acc : 33.935/0.714\n",
      "Epoch 156 ended\n",
      "Training   loss/acc : 357.710/0.801\n",
      "Validation loss/acc : 34.121/0.714\n",
      "Epoch 157 ended\n",
      "Training   loss/acc : 353.191/0.803\n",
      "Validation loss/acc : 34.138/0.715\n",
      "Epoch 158 ended\n",
      "Training   loss/acc : 352.244/0.804\n",
      "Validation loss/acc : 33.697/0.719\n",
      "Epoch 159 ended\n",
      "Training   loss/acc : 350.914/0.804\n",
      "Validation loss/acc : 33.954/0.715\n",
      "Epoch 160 ended\n",
      "Training   loss/acc : 352.751/0.803\n",
      "Validation loss/acc : 34.749/0.705\n",
      "Epoch 161 ended\n",
      "Training   loss/acc : 347.585/0.806\n",
      "Validation loss/acc : 35.086/0.707\n",
      "Epoch 162 ended\n",
      "Training   loss/acc : 347.475/0.808\n",
      "Validation loss/acc : 34.365/0.713\n",
      "Epoch 163 ended\n",
      "Training   loss/acc : 344.570/0.808\n",
      "Validation loss/acc : 34.095/0.715\n",
      "Epoch 164 ended\n",
      "Training   loss/acc : 345.789/0.807\n",
      "Validation loss/acc : 34.002/0.717\n",
      "Epoch 165 ended\n",
      "Training   loss/acc : 344.659/0.808\n",
      "Validation loss/acc : 34.103/0.714\n",
      "Epoch 166 ended\n",
      "Training   loss/acc : 342.348/0.809\n",
      "Validation loss/acc : 34.727/0.709\n",
      "Epoch 167 ended\n",
      "Training   loss/acc : 341.474/0.810\n",
      "Validation loss/acc : 34.479/0.711\n",
      "Epoch 168 ended\n",
      "Training   loss/acc : 339.981/0.810\n",
      "Validation loss/acc : 34.637/0.709\n",
      "Epoch 169 ended\n",
      "Training   loss/acc : 341.765/0.810\n",
      "Validation loss/acc : 34.967/0.708\n",
      "Epoch 170 ended\n",
      "Training   loss/acc : 336.175/0.813\n",
      "Validation loss/acc : 34.658/0.710\n",
      "Epoch 171 ended\n",
      "Training   loss/acc : 338.111/0.813\n",
      "Validation loss/acc : 34.452/0.712\n",
      "Epoch 172 ended\n",
      "Training   loss/acc : 336.220/0.813\n",
      "Validation loss/acc : 34.220/0.715\n",
      "Epoch 173 ended\n",
      "Training   loss/acc : 333.934/0.814\n",
      "Validation loss/acc : 34.531/0.711\n",
      "Epoch 174 ended\n",
      "Training   loss/acc : 333.443/0.815\n",
      "Validation loss/acc : 34.147/0.712\n",
      "Epoch 175 ended\n",
      "Training   loss/acc : 331.402/0.816\n",
      "Validation loss/acc : 35.445/0.710\n",
      "Epoch 176 ended\n",
      "Training   loss/acc : 332.336/0.816\n",
      "Validation loss/acc : 34.718/0.711\n",
      "Epoch 177 ended\n",
      "Training   loss/acc : 330.649/0.816\n",
      "Validation loss/acc : 34.743/0.708\n",
      "Epoch 178 ended\n",
      "Training   loss/acc : 331.061/0.815\n",
      "Validation loss/acc : 34.990/0.711\n",
      "Epoch 179 ended\n",
      "Training   loss/acc : 328.679/0.817\n",
      "Validation loss/acc : 34.291/0.715\n",
      "Epoch 180 ended\n",
      "Training   loss/acc : 326.235/0.819\n",
      "Validation loss/acc : 34.886/0.711\n",
      "Epoch 181 ended\n",
      "Training   loss/acc : 328.299/0.819\n",
      "Validation loss/acc : 34.471/0.715\n",
      "Epoch 182 ended\n",
      "Training   loss/acc : 326.270/0.819\n",
      "Validation loss/acc : 34.870/0.711\n",
      "Epoch 183 ended\n",
      "Training   loss/acc : 326.112/0.818\n",
      "Validation loss/acc : 34.745/0.713\n",
      "Epoch 184 ended\n",
      "Training   loss/acc : 324.485/0.819\n",
      "Validation loss/acc : 34.303/0.714\n",
      "Epoch 185 ended\n",
      "Training   loss/acc : 323.245/0.819\n",
      "Validation loss/acc : 35.173/0.710\n",
      "Epoch 186 ended\n",
      "Training   loss/acc : 323.448/0.819\n",
      "Validation loss/acc : 34.625/0.713\n",
      "Epoch 187 ended\n",
      "Training   loss/acc : 320.282/0.822\n",
      "Validation loss/acc : 34.213/0.717\n",
      "Epoch 188 ended\n",
      "Training   loss/acc : 321.242/0.821\n",
      "Validation loss/acc : 34.924/0.708\n",
      "Epoch 189 ended\n",
      "Training   loss/acc : 318.056/0.823\n",
      "Validation loss/acc : 34.752/0.713\n",
      "Epoch 190 ended\n",
      "Training   loss/acc : 320.987/0.821\n",
      "Validation loss/acc : 34.589/0.716\n",
      "Epoch 191 ended\n",
      "Training   loss/acc : 319.656/0.822\n",
      "Validation loss/acc : 34.954/0.711\n",
      "Epoch 192 ended\n",
      "Training   loss/acc : 316.140/0.824\n",
      "Validation loss/acc : 34.809/0.714\n",
      "Epoch 193 ended\n",
      "Training   loss/acc : 316.307/0.825\n",
      "Validation loss/acc : 35.142/0.714\n",
      "Epoch 194 ended\n",
      "Training   loss/acc : 315.813/0.823\n",
      "Validation loss/acc : 34.434/0.713\n",
      "Epoch 195 ended\n",
      "Training   loss/acc : 312.710/0.825\n",
      "Validation loss/acc : 34.624/0.712\n",
      "Epoch 196 ended\n",
      "Training   loss/acc : 313.428/0.825\n",
      "Validation loss/acc : 34.761/0.713\n",
      "Epoch 197 ended\n",
      "Training   loss/acc : 312.641/0.826\n",
      "Validation loss/acc : 34.869/0.713\n",
      "Epoch 198 ended\n",
      "Training   loss/acc : 311.422/0.827\n",
      "Validation loss/acc : 34.584/0.715\n",
      "Epoch 199 ended\n",
      "Training   loss/acc : 311.494/0.827\n",
      "Validation loss/acc : 34.685/0.715\n",
      "Epoch 200 ended\n",
      "Training   loss/acc : 311.003/0.828\n",
      "Validation loss/acc : 34.750/0.716\n"
     ]
    }
   ],
   "source": [
    "d_model, tr_loss, tr_ac, val_loss, val_ac = cifar10_experiment(\"dropout\", train_dataloader, valid_dataloader, dropout_rate=0.1, dropout_type='f', lr=0.01, momentum=0.9, epoch=200, num_sample=3, valid_num_sample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(d_model, \"dropout.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
